{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Cancer Cell Segmentation Nir.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "iadNjTwWwwTY",
        "MzeZnwcM3BUg",
        "rF-G30oySA83"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Noy-Bo/Cancer-Cell-Segmentation/blob/master/Cancer_Cell_Segmentation_Nir.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iadNjTwWwwTY"
      },
      "source": [
        "# Mounting Google Drive\n",
        "\n",
        "mounting drive, setting root path to PanNuke dataset, setting device to cuda, checking out GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-p9G0v8wsgvk",
        "outputId": "3c02d969-e1af-4c83-b676-371a480180ef"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68JnPIOOtM-f"
      },
      "source": [
        "import os\n",
        "root_path = '/content/gdrive/MyDrive/PanNuke' \n",
        "os.chdir(root_path)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1T8t0kxy7Z3B",
        "outputId": "53b374dc-adbe-455b-8be0-6d5328aa46de"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cpu')\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "print(device)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAtxgDvQ7eDJ",
        "outputId": "43fb8bb0-7a2a-4d57-93e0-fc9f4a4faa1f"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jul 22 13:08:20 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    27W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbWI64yG2wEw"
      },
      "source": [
        "# **Dataset**\n",
        "loading files, creating dataloaders, etc..\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y25bHcYNtcmu"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "# calc normalization values of given data loader\n",
        "def calc_normalization(data_loader):\n",
        "    pop_mean = []\n",
        "    pop_std0 = []\n",
        "    pop_std1 = []\n",
        "    for idx_batch, (images, masks) in enumerate(data_loader, 0):\n",
        "     # shape (batch_size, 3, height, width)\n",
        "     numpy_image = images.numpy()\n",
        "\n",
        "     # shape (3,)\n",
        "     batch_mean = np.mean(numpy_image, axis=(0, 2, 3))\n",
        "     batch_std0 = np.std(numpy_image, axis=(0, 2, 3))\n",
        "     batch_std1 = np.std(numpy_image, axis=(0, 2, 3), ddof=1)\n",
        "\n",
        "     pop_mean.append(batch_mean/255)\n",
        "     pop_std0.append(batch_std0/255)\n",
        "     pop_std1.append(batch_std1/255)\n",
        "\n",
        "    # shape (num_iterations, 3) -> (mean across 0th axis) -> shape (3,)\n",
        "    pop_mean = np.array(pop_mean).mean(axis=0)\n",
        "    print(pop_mean)\n",
        "    pop_std0 = np.array(pop_std0).mean(axis=0)\n",
        "    print(pop_std0)\n",
        "    pop_std1 = np.array(pop_std1).mean(axis=0)\n",
        "    print(pop_std1)\n",
        "\n",
        "\n",
        "# VISUALIZING SAMPLES - designed for input that is the dataloader output, for raw input cancel moveaxis\n",
        "def vis_sample(image, masks):\n",
        "    fig, axs = plt.subplots(1,7)\n",
        "    fig.set_size_inches(18.5, 3.2)\n",
        "    fig.suptitle('Ground Truth', fontsize=18)\n",
        "    #fig.tight_layout()\n",
        "    axs[0].imshow(np.moveaxis(image.numpy().astype(np.uint8),0,-1)); axs[0].set_title(\"Sample\")\n",
        "    masks = np.moveaxis(masks.numpy().astype(np.uint8),0,-1)\n",
        "    axs[1].imshow(masks[:,:,0], cmap=\"gray\"); axs[1].set_title(\"Neoplastic cells\")\n",
        "    axs[2].imshow(masks[:,:,1], cmap=\"gray\"); axs[2].set_title(\"Inflammatory\")\n",
        "    axs[3].imshow(masks[:,:,2], cmap=\"gray\"); axs[3].set_title(\"Connective/Soft tissue cells\")\n",
        "    axs[4].imshow(masks[:,:,3], cmap=\"gray\"); axs[4].set_title(\"Dead Cells\")\n",
        "    axs[5].imshow(masks[:,:,4], cmap=\"gray\"); axs[5].set_title(\"Epithelial\")\n",
        "    #axs[6].imshow(masks[:,:,5], cmap=\"gray\"); axs[6].set_title(\"Background\")\n",
        "    plt.show()\n",
        "\n",
        "# VISUALIZING PREDICTIONS - visualizing network output on a random sample.\n",
        "def vis_predictions():\n",
        "    image_gt_batch, masks_gt_batch = iter(training_loader_1).next()\n",
        "    pred_masks_batch = model(image_gt_batch)\n",
        "    for i in range(image_gt_batch.shape[0]): # looping on batch_size\n",
        "      print(\"\\n\\n\")\n",
        "      print(\"\\t\\t\\t\\t\\t\\t\\t\\t SAMPLE: {}\".format(str(i)))\n",
        "      image = image_gt_batch[i,...] # 3,256,256\n",
        "      pred_masks = pred_masks_batch[i,...] # 6,256,256\n",
        "\n",
        "      fig, axs = plt.subplots(1,7)\n",
        "      fig.set_size_inches(18.5, 3.3)\n",
        "      fig.suptitle('Prediction', fontsize=18)\n",
        "      #fig.tight_layout()\n",
        "      axs[0].imshow(np.moveaxis(image.numpy().astype(np.uint8),0,-1)); axs[0].set_title(\"Sample\")\n",
        "      axs[1].imshow(pred_masks[0,:,:].cpu().detach().numpy(), cmap=\"gray\"); axs[1].set_title(\"Neoplastic cells\")\n",
        "      axs[2].imshow(pred_masks[1,:,:].cpu().detach().numpy(), cmap=\"gray\"); axs[2].set_title(\"Inflammatory\")\n",
        "      axs[3].imshow(pred_masks[2,:,:].cpu().detach().numpy(), cmap=\"gray\"); axs[3].set_title(\"Connective/Soft tissue cells\")\n",
        "      axs[4].imshow(pred_masks[3,:,:].cpu().detach().numpy(), cmap=\"gray\"); axs[4].set_title(\"Dead Cells\")\n",
        "      axs[5].imshow(pred_masks[4,:,:].cpu().detach().numpy(), cmap=\"gray\"); axs[5].set_title(\"Epithelial\")\n",
        "      #axs[6].imshow(pred_masks[5,:,:].cpu().detach().numpy(), cmap=\"gray\"); axs[6].set_title(\"Background\")\n",
        "      plt.show()\n",
        "\n",
        "      vis_sample(image,masks_gt_batch[i,...]) # visualizing gt (ground truth)\n",
        "\n",
        "\n",
        "# LOADING DATASET\n",
        "def load_dataset(dir_root, dir_images, dir_masks, training_size=0.8):\n",
        "\n",
        "    train_set = PanNuke(dir_root, dir_images, dir_masks, train=True)\n",
        "\n",
        "    # Splitting train into train/val\n",
        "    permutations = torch.randperm(len(train_set))\n",
        "    split = int(np.floor(training_size * len(train_set)))\n",
        "    training_subset = SubsetRandomSampler(permutations[:split])\n",
        "    validation_subset = SubsetRandomSampler(permutations[split:])\n",
        "\n",
        "    # Apply DataLoader over train val and test data\n",
        "    train_loader = DataLoader(train_set, sampler=training_subset, batch_size=1, num_workers=4)\n",
        "    validation_loader = DataLoader(train_set, sampler=validation_subset, batch_size=1, num_workers=4)\n",
        "\n",
        "    return train_loader, validation_loader\n",
        "\n",
        "\n",
        "\n",
        "# DATASET CLASS\n",
        "class PanNuke(Dataset):\n",
        "    def __init__(self, dir_root, dir_images, dir_masks, val=False, train=False, test=False):\n",
        "        self.images = np.load(dir_root+dir_images, mmap_mode='r')\n",
        "        self.images = np.moveaxis(self.images, -1, 1)\n",
        "        #self.images = np.copy(self.images)\n",
        "        self.masks = np.load(dir_root+dir_masks, mmap_mode='r')\n",
        "        self.masks = np.moveaxis(self.masks, -1, 1)\n",
        "        #self.masks = np.copy(self.masks)\n",
        "        self.masks = self.masks[:,:5,...]\n",
        "\n",
        "        self.transforms = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            #transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n",
        "            #transforms.Normalize(mean=[0.73952988, 0.5701334, 0.702605], std=[0.18024648, 0.21097612, 0.16465892 ])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.images.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = np.copy(self.images[idx, ...])\n",
        "        img = self.transforms(img.transpose())\n",
        "        masks = np.copy(self.masks[idx, ...])\n",
        "        masks = np.ceil(masks/1000)\n",
        "        return img, masks\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzeZnwcM3BUg"
      },
      "source": [
        "# **Model**\n",
        "basic UNet model configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cyi6s5FNtjrW"
      },
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def double_conv(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.dblock1 = double_conv(3, 128)\n",
        "        self.dblock2 = double_conv(128, 256)\n",
        "        self.dblock3 = double_conv(256, 512)\n",
        "        self.dblock4 = double_conv(512, 1024)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "        self.dblock5 = double_conv(512 + 1024, 512)\n",
        "        self.dblock6 = double_conv(256 + 512, 256)\n",
        "        self.dblock7 = double_conv(256 + 128, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.last_layer = nn.Conv2d(128, 512, 1)\n",
        "        self.last_layer_rly = nn.Conv2d(512,5,1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv1 = self.dblock1(x)\n",
        "        x = self.pool(conv1)\n",
        "\n",
        "        conv2 = self.dblock2(x)\n",
        "        x = self.pool(conv2)\n",
        "\n",
        "        conv3 = self.dblock3(x)\n",
        "        x = self.pool(conv3)\n",
        "\n",
        "        conv4 = self.dblock4(x)\n",
        "\n",
        "        x = self.upsample(conv4)\n",
        "\n",
        "        x = torch.cat([x, conv3], dim=1)\n",
        "\n",
        "        x = self.dblock5(x)\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat([x, conv2], dim=1)\n",
        "\n",
        "        x = self.dblock6(x)\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat([x, conv1], dim=1)\n",
        "\n",
        "        x = self.dblock7(x)\n",
        "\n",
        "        x = self.last_layer(x)\n",
        "        out = self.last_layer_rly(x)\n",
        "        # out = self.sigmoid(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        features = init_features\n",
        "        self.dropout = nn.Dropout2d()\n",
        "        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(\n",
        "            features * 16, features * 8, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder4 = UNet._block((features * 8) * 2, features * 8, name=\"dec4\")\n",
        "        self.upconv3 = nn.ConvTranspose2d(\n",
        "            features * 8, features * 4, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder3 = UNet._block((features * 4) * 2, features * 4, name=\"dec3\")\n",
        "        self.upconv2 = nn.ConvTranspose2d(\n",
        "            features * 4, features * 2, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder2 = UNet._block((features * 2) * 2, features * 2, name=\"dec2\")\n",
        "        self.upconv1 = nn.ConvTranspose2d(\n",
        "            features * 2, features, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n",
        "\n",
        "        self.conv_masks = nn.Conv2d(in_channels=features, out_channels=5, kernel_size=1)\n",
        "\n",
        "        # self.conv = nn.Conv2d(\n",
        "        #     in_channels=features, out_channels=out_channels, kernel_size=1\n",
        "        # )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        enc1 = self.encoder1(x)\n",
        "        enc1 = self.dropout(enc1)\n",
        "        enc2 = self.encoder2(self.pool1(enc1))\n",
        "        enc2 = self.dropout(enc2)\n",
        "        enc3 = self.encoder3(self.pool2(enc2))\n",
        "        enc3 = self.dropout(enc3)\n",
        "        enc4 = self.encoder4(self.pool3(enc3))\n",
        "        enc4 = self.dropout(enc4)\n",
        "\n",
        "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
        "        bottleneck = self.dropout(bottleneck)\n",
        "\n",
        "        dec4 = self.upconv4(bottleneck)\n",
        "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
        "        dec4 = self.decoder4(dec4)\n",
        "        dec4 = self.dropout(dec4)\n",
        "        dec3 = self.upconv3(dec4)\n",
        "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
        "        dec3 = self.decoder3(dec3)\n",
        "        dec3 = self.dropout(dec3)\n",
        "        dec2 = self.upconv2(dec3)\n",
        "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
        "        dec2 = self.decoder2(dec2)\n",
        "        dec2 = self.dropout(dec2)\n",
        "        dec1 = self.upconv1(dec2)\n",
        "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
        "        dec1 = self.decoder1(dec1)\n",
        "        dec1 = self.dropout(dec1)\n",
        "        #return torch.sigmoid(self.conv(dec1))\n",
        "        return torch.sigmoid(self.conv_masks(dec1))\n",
        "\n",
        "    @staticmethod\n",
        "    def _block(in_channels, features, name):\n",
        "        return nn.Sequential(\n",
        "            OrderedDict(\n",
        "                [\n",
        "                    (\n",
        "                        name + \"conv1\",\n",
        "                        nn.Conv2d(\n",
        "                            in_channels=in_channels,\n",
        "                            out_channels=features,\n",
        "                            kernel_size=3,\n",
        "                            padding=1,\n",
        "                            bias=False,\n",
        "                        ),\n",
        "                    ),\n",
        "                    (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n",
        "                    (name + \"relu1\", nn.ReLU(inplace=True)),\n",
        "                    (\n",
        "                        name + \"conv2\",\n",
        "                        nn.Conv2d(\n",
        "                            in_channels=features,\n",
        "                            out_channels=features,\n",
        "                            kernel_size=3,\n",
        "                            padding=1,\n",
        "                            bias=False,\n",
        "                        ),\n",
        "                    ),\n",
        "                    (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n",
        "                    (name + \"relu2\", nn.ReLU(inplace=True)),\n",
        "                ]\n",
        "            )\n",
        "        )"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB1G6fPU3N7I"
      },
      "source": [
        "# **Train Utilities**\n",
        "loss functions configurations, train epoch function ..\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsxsKcFftmZu",
        "outputId": "d86533f2-3044-42f7-9e07-8529da1923c4"
      },
      "source": [
        "import sys\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import tqdm\n",
        "\n",
        "device = torch.device('cpu')\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "print(device)\n",
        "\n",
        "def dice_loss(predict, target):\n",
        "    smooth = 1.\n",
        "    loss = 0.\n",
        "    for c in range(predict.shape[1]):\n",
        "        iflat = predict[:, c, ...].contiguous().view(-1)\n",
        "        tflat = target[:, c, ...].contiguous().view(-1)\n",
        "        intersection = (iflat * tflat).sum()\n",
        "\n",
        "        loss +=  (1 - ((2. * intersection + smooth) /\n",
        "                          (iflat.sum() + tflat.sum() + smooth)))\n",
        "    return loss\n",
        "def BCE(predict, target):\n",
        "    loss_func = nn.BCELoss()\n",
        "    softmax = torch.nn.Softmax(dim=0)\n",
        "    assert predict.shape[0] == target.shape[0], \"predict & target batch size don't match\"\n",
        "    loss = 0\n",
        "    predict = predict.permute(0,2,3,1)\n",
        "    predict = predict.flatten()\n",
        "    predict = predict.reshape(5,-1)\n",
        "    predict = softmax(predict)\n",
        "    target = target.permute(0,2,3,1)\n",
        "    target = target.flatten()\n",
        "    target = target.reshape(5,-1)\n",
        "\n",
        "    loss += loss_func(predict,target)\n",
        "    return loss\n",
        "class BinaryDiceLoss(nn.Module):\n",
        "    \"\"\"Dice loss of binary class\n",
        "    Args:\n",
        "        smooth: A float number to smooth loss, and avoid NaN error, default: 1\n",
        "        p: Denominator value: \\sum{x^p} + \\sum{y^p}, default: 2\n",
        "        predict: A tensor of shape [N, *]\n",
        "        target: A tensor of shape same with predict\n",
        "        reduction: Reduction method to apply, return mean over batch if 'mean',\n",
        "            return sum if 'sum', return a tensor of shape [N,] if 'none'\n",
        "    Returns:\n",
        "        Loss tensor according to arg reduction\n",
        "    Raise:\n",
        "        Exception if unexpected reduction\n",
        "    \"\"\"\n",
        "    def __init__(self, smooth=1, p=2, reduction='sum'):\n",
        "        super(BinaryDiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "        self.p = p\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, predict, target):\n",
        "        assert predict.shape[0] == target.shape[0], \"predict & target batch size don't match\"\n",
        "        loss = 0\n",
        "        for b_idx in range(predict.shape[0]):\n",
        "            for m_idx in range(predict.shape[1]):\n",
        "                predict_i = predict[b_idx,m_idx,...].contiguous().view(-1)\n",
        "                target_i = target[b_idx,m_idx,...].contiguous().view(-1)\n",
        "\n",
        "                num = torch.sum(torch.mul(predict_i, target_i), dim=0) + self.smooth\n",
        "                den = torch.sum(predict_i.pow(self.p) + target_i.pow(self.p), dim=0) + self.smooth\n",
        "\n",
        "                loss = loss +  1 - num / den\n",
        "\n",
        "        # predict = predict.contiguous().view(predict.shape[0], -1)\n",
        "        # target = target.contiguous().view(target.shape[0], -1)\n",
        "\n",
        "        # num = torch.sum(torch.mul(predict, target), dim=1) + self.smooth\n",
        "        # den = torch.sum(predict.pow(self.p) + target.pow(self.p), dim=1) + self.smooth\n",
        "        #\n",
        "        # loss = 1 - num / den\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return loss.sum()\n",
        "        elif self.reduction == 'none':\n",
        "            return loss\n",
        "        else:\n",
        "            raise Exception('Unexpected reduction {}'.format(self.reduction))\n",
        "\n",
        "\n",
        "def train_epoch(training_loader, model, optimizer, loss_function):\n",
        "\n",
        "    losses = []\n",
        "    model.train()\n",
        "   \n",
        "    with tqdm.tqdm(total=len(training_loader), file=sys.stdout) as pbar:\n",
        "        for idx_batch, (images, masks) in enumerate(training_loader, start=1):\n",
        "\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            # calculate output\n",
        "            y_hat = model(images)\n",
        "\n",
        "            # calculate loss now:\n",
        "            optimizer.zero_grad()\n",
        "            loss = loss_function(y_hat, masks)\n",
        "            loss.backward()\n",
        "\n",
        "            # optimizing weights\n",
        "            optimizer.step()\n",
        "\n",
        "            # update loss bar\n",
        "            losses.append(loss.detach())\n",
        "            pbar.update();\n",
        "            pbar.set_description(f'train loss={losses[-1]:.3f}')\n",
        "        mean_loss = torch.mean(torch.FloatTensor(losses))\n",
        "        pbar.set_description(f'train loss={mean_loss:.3f}')\n",
        "\n",
        "        images = None\n",
        "        masks = None\n",
        "    return [mean_loss]\n",
        "\n",
        "    \n",
        "def eval_loss_epoch(training_loader, model, loss_function):\n",
        "\n",
        "    losses = []\n",
        "    model.eval()\n",
        "    with tqdm.tqdm(total=len(training_loader), file=sys.stdout) as pbar:\n",
        "        for idx_batch, (images, masks) in enumerate(training_loader, start=1):\n",
        "\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            # calculate output\n",
        "            y_hat = model(images)\n",
        "\n",
        "            # calculate loss now:\n",
        "            loss = loss_function(y_hat, masks)\n",
        "\n",
        "            # optimizing weights\n",
        "\n",
        "            # update loss bar\n",
        "            losses.append(loss.detach())\n",
        "            pbar.update();\n",
        "            pbar.set_description(f'val loss={losses[-1]:.3f}')\n",
        "\n",
        "            images = None\n",
        "            masks = None\n",
        "        mean_loss = torch.mean(torch.FloatTensor(losses))\n",
        "        pbar.set_description(f'val loss={mean_loss:.3f}')\n",
        "\n",
        "    return [mean_loss]\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G3F2lwvZ2rW"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgCVfGIl3jiZ"
      },
      "source": [
        "# **Main (training)**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-t83yC39t7sH",
        "outputId": "a0277efc-b0a5-4c9c-cfbb-3dda50d9cf51"
      },
      "source": [
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as TNF\n",
        "import torch.nn.modules\n",
        "from torch import optim\n",
        "import time\n",
        "#import dataset as pnk\n",
        "from torch.utils.data import DataLoader\n",
        "from torchsummary import summary\n",
        "import tqdm\n",
        "\n",
        "# Small useful components:\n",
        "# this class is to be able to use TNF.interpole within nn.Sequential()\n",
        "class Interpolate(nn.Module):\n",
        "    def __init__(self, size, mode):\n",
        "        super(Interpolate, self).__init__()\n",
        "        self.interp = TNF.interpolate\n",
        "        self.size = size\n",
        "        self.mode = mode\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.interp(x, size=self.size, mode=self.mode, align_corners=False)\n",
        "\n",
        "# Create the Micro-Net components:\n",
        "class Group1_B1(nn.Module):\n",
        "    def __init__(self, in_channels=3):\n",
        "        super(Group1_B1, self).__init__()\n",
        "\n",
        "        self.sub_block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=5, stride=1, padding=0, bias=False), # kernel=5 since we start with 256 imgs, where in paper it's 252\n",
        "            nn.BatchNorm2d(64), # out_channels=64\n",
        "            nn.Tanh(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=5, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(64),  # REMOVE?\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "        ) #124,124,64\n",
        "\n",
        "        self.sub_block2 = nn.Sequential(\n",
        "            Interpolate(size=(128,128),mode='bicubic'),\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=3, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(64),  # out_channels=64\n",
        "            nn.Tanh(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=0, bias=False),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, orig_input):\n",
        "        sub_block1 = self.sub_block1(orig_input) # recall it outputs: B,CH,HEIGHT,WIDTH\n",
        "        sub_block2 = self.sub_block2(orig_input)\n",
        "        B1 = torch.cat((sub_block1,sub_block2), dim=1) # concat alongside channels dim'\n",
        "        return B1\n",
        "\n",
        "class Group1_B2(nn.Module):\n",
        "    def __init__(self, in_channels=128):\n",
        "        super(Group1_B2, self).__init__()\n",
        "        self.sub_block1 = nn.Sequential(  # gets 124^2, ch=128,   outputs: 60^2, ch=128\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=128, kernel_size=3, stride=1, padding=0, bias=True), # bias=True since no BN is applied.\n",
        "            nn.Tanh(),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=0, bias=True),\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.sub_block2 = nn.Sequential( # gets 256^2, ch=3  outputs: 60^2, ch=128\n",
        "            Interpolate(size=(64, 64), mode='bicubic'),\n",
        "            nn.Conv2d(in_channels=3, out_channels=128, kernel_size=3, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Tanh(),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=0, bias=False),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, B1_input, orig_input):\n",
        "        sub_block1 = self.sub_block1(B1_input)\n",
        "        sub_block2 = self.sub_block2(orig_input)\n",
        "        B2 = torch.cat((sub_block1, sub_block2), dim=1)  # concat alongside channels dim'\n",
        "        return B2\n",
        "\n",
        "class Group1_B3(nn.Module):\n",
        "    def __init__(self, in_channels=256):\n",
        "        super(Group1_B3, self).__init__()\n",
        "        self.sub_block1 = nn.Sequential(  # gets 60^2, ch=256,   outputs: 28^2, ch=256\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=256, kernel_size=3, stride=1, padding=0, bias=True), # bias=True since no BN is applied.\n",
        "            nn.Tanh(),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=0, bias=True),\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.sub_block2 = nn.Sequential( # gets 256^2, ch=3  outputs: 28^2, ch=256\n",
        "            Interpolate(size=(32, 32), mode='bicubic'),\n",
        "            nn.Conv2d(in_channels=3, out_channels=256, kernel_size=3, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Tanh(),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=0, bias=False),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, B2_input, orig_input):\n",
        "        sub_block1 = self.sub_block1(B2_input)\n",
        "        sub_block2 = self.sub_block2(orig_input)\n",
        "        B3 = torch.cat((sub_block1, sub_block2), dim=1)  # concat alongside channels dim'\n",
        "        return B3\n",
        "\n",
        "class Group1_B4(nn.Module):\n",
        "    def __init__(self, in_channels=512):\n",
        "        super(Group1_B4, self).__init__()\n",
        "        self.sub_block1 = nn.Sequential(  # gets 28^2, ch=512,   outputs: 12^2, ch=512\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=512, kernel_size=3, stride=1, padding=0, bias=True), # bias=True since no BN is applied.\n",
        "            nn.Tanh(),\n",
        "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=0, bias=True),\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.sub_block2 = nn.Sequential( # gets 256^2, ch=3  outputs: 12^2, ch=512\n",
        "            Interpolate(size=(16, 16), mode='bicubic'),\n",
        "            nn.Conv2d(in_channels=3, out_channels=512, kernel_size=3, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Tanh(),\n",
        "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=0, bias=False),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, B3_input, orig_input):\n",
        "        sub_block1 = self.sub_block1(B3_input)\n",
        "        sub_block2 = self.sub_block2(orig_input)\n",
        "        B4 = torch.cat((sub_block1, sub_block2), dim=1)  # concat alongside channels dim'\n",
        "        return B4\n",
        "\n",
        "class Group2_B5(nn.Module):\n",
        "    def __init__(self, in_channels=1024):\n",
        "        super(Group2_B5, self).__init__()\n",
        "        self.sub_block = nn.Sequential(  # gets 12^2, ch=1024,   outputs: 8^2, ch=2048\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=2048, kernel_size=3, stride=1, padding=0, bias=True), # bias=True since no BN is applied.\n",
        "            nn.Tanh(),\n",
        "            nn.Conv2d(in_channels=2048, out_channels=2048, kernel_size=3, stride=1, padding=0, bias=True),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, B4_input):\n",
        "        B5 = self.sub_block(B4_input)\n",
        "        return B5\n",
        "\n",
        "class Group3_Bi(nn.Module):\n",
        "    def __init__(self, in_channels_prev_b, in_channels_g1):\n",
        "        super(Group3_Bi, self).__init__()\n",
        "        # ------------------- UNSURE HERE REGARDING THE 1ST SUBBLOCK, WE DECONV UP TO 16 THEN CONV TWICE TO 8 THEN AGAIN DECONV UP TO 16?????\n",
        "        # First-part of the block:\n",
        "        self.sub_block1 = nn.Sequential(  # gets size^2, #channels, outputs (size*2)^2, #channels/2\n",
        "            nn.ConvTranspose2d(in_channels=in_channels_prev_b, out_channels=round(in_channels_prev_b / 2),kernel_size=2, stride=2, padding=0), # double x=h,w to 2x X 2x\n",
        "            nn.Conv2d(in_channels=round(in_channels_prev_b / 2), out_channels=round(in_channels_prev_b / 2), kernel_size=3, stride=1, padding=0), # turn to 2x-2 X 2x-2\n",
        "            # nn.BatchNorm2d(in_channels_prev_b/2),\n",
        "            nn.Tanh(),\n",
        "            nn.Conv2d(in_channels=round(in_channels_prev_b / 2), out_channels=round(in_channels_prev_b / 2), kernel_size=3, stride=1, padding=0),  # turn to 2x -4 X 2x-4\n",
        "            # nn.BatchNorm2d(in_channels_prev_b/2),\n",
        "            nn.Tanh(),\n",
        "            nn.ConvTranspose2d(in_channels=round(in_channels_prev_b / 2), out_channels=round(in_channels_prev_b / 2), kernel_size=5, stride=1, padding=0),  # turn back to 2x X 2x,\n",
        "        )\n",
        "\n",
        "        # Mid-part of the block:\n",
        "        self.sub_block2 = nn.ConvTranspose2d(in_channels=in_channels_g1, out_channels=in_channels_g1, kernel_size=5, stride=1, padding=0) # it upsamples by 4 only\n",
        "\n",
        "        # Third-part of the block:\n",
        "        self.sub_block3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=round(in_channels_g1*2), out_channels=in_channels_g1, kernel_size=3, stride=1, padding=1), # same conv\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, g1_input, prev_b_input):\n",
        "        sub_block1 = self.sub_block1(prev_b_input)\n",
        "        sub_block2 = self.sub_block2(g1_input)\n",
        "        sub_block3 = torch.cat((sub_block1, sub_block2), dim=1)  # concat alongside channels dim'\n",
        "        Bi = self.sub_block3(sub_block3)\n",
        "        return Bi\n",
        "\n",
        "class Group4_Pa1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Group4_Pa1, self).__init__()\n",
        "        self.sub_block1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2, stride=2), #upsample by 2x\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),  #UNLIKE THE PAPER, we'll use same convs, in order to get final-output= 256x256 as out data imgs\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "        self.sub_block2 = nn.Sequential(\n",
        "            nn.Dropout2d(p=0.5),\n",
        "            nn.Conv2d(in_channels=64, out_channels=6, kernel_size=3, stride=1, padding=1), #unlike paper ^^\n",
        "            #nn.Tanh(inplace=True), # Since it's output layer ^^..\n",
        "        )\n",
        "\n",
        "    def forward(self, b9_input):\n",
        "        x1 = self.sub_block1(b9_input) # this also goes onwards to Group5\n",
        "        pa1 = self.sub_block2(x1)\n",
        "        return pa1, x1\n",
        "\n",
        "class Group4_Pa2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Group4_Pa2, self).__init__()\n",
        "        self.sub_block1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=4), #upsample by 4x\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),  #UNLIKE THE PAPER, we'll use same convs, in order to get final-output= 256x256 as out data imgs\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "        self.sub_block2 = nn.Sequential(\n",
        "            nn.Dropout2d(p=0.5),\n",
        "            nn.Conv2d(in_channels=128, out_channels=6, kernel_size=3, stride=1, padding=1), #unlike paper ^^\n",
        "            #nn.Tanh(inplace=True), # Since it's output layer ^^..\n",
        "        )\n",
        "\n",
        "    def forward(self, b8_input):\n",
        "        x2 = self.sub_block1(b8_input) # this also goes onwards to Group5\n",
        "        pa2 = self.sub_block2(x2)\n",
        "        return pa2, x2\n",
        "\n",
        "class Group4_Pa3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Group4_Pa3, self).__init__()\n",
        "        self.sub_block1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=8, stride=8), #upsample by 8x\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),  #UNLIKE THE PAPER, we'll use same convs, in order to get final-output= 256x256 as out data imgs\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "        self.sub_block2 = nn.Sequential(\n",
        "            nn.Dropout2d(p=0.5),\n",
        "            nn.Conv2d(in_channels=256, out_channels=6, kernel_size=3, stride=1, padding=1), #unlike paper ^^\n",
        "            #nn.Tanh(inplace=True), # Since it's output layer ^^..\n",
        "        )\n",
        "\n",
        "    def forward(self, b7_input):\n",
        "        x3 = self.sub_block1(b7_input) # this also goes onwards to Group5\n",
        "        pa3 = self.sub_block2(x3)\n",
        "        return pa3, x3\n",
        "\n",
        "class Group5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Group5, self).__init__()\n",
        "        self.sub_block = nn.Sequential( #unlike the paper, the input for this stage is 256x256, 448(after concat)\n",
        "            nn.Dropout2d(p=0.5),\n",
        "            nn.Conv2d(in_channels=448, out_channels=6, kernel_size=3, stride=1, padding=1), #unlike paper ^^\n",
        "        )\n",
        "\n",
        "    def forward(self, x1, x2, x3):\n",
        "        x = torch.cat((x1, x2, x3), dim=1) # concat x1,x2,x3 alongside channels dim'\n",
        "        p0 = self.sub_block(x)\n",
        "        return p0\n",
        "\n",
        "class MicroNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MicroNet, self).__init__()\n",
        "        self.group1_b1 = Group1_B1()\n",
        "        self.group1_b2 = Group1_B2()\n",
        "        self.group1_b3 = Group1_B3()\n",
        "        self.group1_b4 = Group1_B4()\n",
        "        self.group2 = Group2_B5()\n",
        "        self.group3_b6 = Group3_Bi(in_channels_prev_b=2048, in_channels_g1=1024)\n",
        "        self.group3_b7 = Group3_Bi(in_channels_prev_b=1024, in_channels_g1=512)\n",
        "        self.group3_b8 = Group3_Bi(in_channels_prev_b=512, in_channels_g1=256)\n",
        "        self.group3_b9 = Group3_Bi(in_channels_prev_b=256, in_channels_g1=128)\n",
        "        self.group4_pa1 = Group4_Pa1()\n",
        "        self.group4_pa2 = Group4_Pa2()\n",
        "        self.group4_pa3 = Group4_Pa3()\n",
        "        self.group5 = Group5()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Propagate through G1:\n",
        "        b1 = self.group1_b1(x)\n",
        "        b2 = self.group1_b2(b1, x)\n",
        "        b3 = self.group1_b3(b2, x)\n",
        "        b4 = self.group1_b4(b3, x)\n",
        "\n",
        "        # Propagate through G2:\n",
        "        b5 = self.group2(b4)\n",
        "\n",
        "        # Propagate through G3:\n",
        "        b6 = self.group3_b6(b4, b5)\n",
        "        b7 = self.group3_b7(b3, b6)\n",
        "        b8 = self.group3_b8(b2, b7)\n",
        "        b9 = self.group3_b9(b1, b8)\n",
        "\n",
        "        # Propagate through G4:\n",
        "        pa1, x1 = self.group4_pa1(b9)\n",
        "        pa2, x2 = self.group4_pa2(b8)\n",
        "        pa3, x3 = self.group4_pa3(b7)\n",
        "\n",
        "        # Propagate through G5:\n",
        "        p0 = self.group5(x1, x2, x3)\n",
        "\n",
        "        return p0, pa1, pa2, pa3  # recall that p0 = main output, pa1,pa2,pa3 = auxiliary outputs\n",
        "\n",
        "\n",
        "# def load_dataset(batch_size, shuffle_flag, num_workers, data_dir, transforms=None):\n",
        "#     dataset = pnk.PanNukeDataset(data_dir)\n",
        "#     data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle_flag)#, num_workers=num_workers)\n",
        "#     return data_loader\n",
        "\n",
        "def train(model, loader, opt, criterion,scheduler, epoch):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    # Train the model (turn training-mode on..)\n",
        "    model.train()\n",
        "\n",
        "    with tqdm.tqdm(total=len(loader), file=sys.stdout) as pbar:\n",
        "        for (images, masks) in loader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            # reinitialize gradients..\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # Calculation of loss (according to paper):\n",
        "            p0, pa1, pa2, pa3 = model(images) # Pi.shape, 256x256x5\n",
        "            masks = torch.argmax(masks, dim=1)\n",
        "            #loss = criterion(output, masks)\n",
        "            l0 = criterion(p0, masks)\n",
        "            l1 = criterion(pa1, masks)\n",
        "            l2 = criterion(pa2, masks)\n",
        "            l3 = criterion(pa3, masks)\n",
        "            loss = l0+(l1+l2+l3)/epoch\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            #acc = calculate_accuracy(output, labels)\n",
        "\n",
        "            # update weights according to gradients\n",
        "            opt.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            #epoch_acc += acc.item\n",
        "\n",
        "            pbar.update()\n",
        "            pbar.set_description(f'train loss={loss.item():.3f}')\n",
        "\n",
        "        pbar.set_description(f'train loss={epoch_loss / len(loader):.3f}')\n",
        "    scheduler.step()\n",
        "\n",
        "    return epoch_loss / len(loader), epoch_acc / len(loader)\n",
        "\n",
        "\n",
        "# ================= MAIN =====================\n",
        "# Variables:\n",
        "BATCH_SIZE = 6\n",
        "NUM_WORKERS = 4\n",
        "EPOCHS = 50\n",
        "LEARNING_RATE=0.001\n",
        "\n",
        "# train_dir = './/Final_Dataset/train_pickled_data'\n",
        "# val_dir = './/Final_Dataset/val_pickled_data'\n",
        "# test_dir = './/Final_Dataset/test_pickled_data'\n",
        "\n",
        "# Set up device:\n",
        "device = torch.device('cpu')\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "print(device)\n",
        "\n",
        "# Grab loaders:\n",
        "# train_loader = load_dataset(BATCH_SIZE, shuffle_flag=True, num_workers=NUM_WORKERS, data_dir=train_dir)\n",
        "# val_loader = load_dataset(BATCH_SIZE, shuffle_flag=True, num_workers=NUM_WORKERS, data_dir=val_dir)\n",
        "# test_loader = load_dataset(BATCH_SIZE, shuffle_flag=True, num_workers=NUM_WORKERS, data_dir=test_dir)\n",
        "\n",
        "# creating data loaders\n",
        "images_dir = 'Images 1/images.npy'\n",
        "masks_dir = 'Masks 1/masks.npy'\n",
        "training_loader_1, _ = load_dataset(dir_root='/content/gdrive/MyDrive/PanNuke/', dir_masks=masks_dir, dir_images=images_dir, training_size=1)\n",
        "images_dir = 'Images 3/images.npy'\n",
        "masks_dir = 'Masks 3/masks.npy'\n",
        "training_loader_2, _ = load_dataset(dir_root='/content/gdrive/MyDrive/PanNuke/', dir_masks=masks_dir, dir_images=images_dir, training_size=1)\n",
        "images_dir = 'Images 2/images.npy'\n",
        "masks_dir = 'Masks 2/masks.npy'\n",
        "validation_loader, test_loader = load_dataset(dir_root='/content/gdrive/MyDrive/PanNuke/', dir_masks=masks_dir, dir_images=images_dir, training_size=0.5)\n",
        "\n",
        "\n",
        "\n",
        "model = MicroNet().to(device)\n",
        "summary(model, (3, 256, 256))\n",
        "model.double()\n",
        "\n",
        "# Define optimizer and criterion functions   - IMPROVE... LEARN HP'S\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999), eps=1e-08)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5, last_epoch=-1, verbose=False)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"Epoch-%d: \" % (epoch))\n",
        "\n",
        "    train_start_time = time.monotonic()\n",
        "    train_loss, train_acc = train(model, training_loader_1, optimizer, loss_func, scheduler, epoch+1)\n",
        "    train_loss, train_acc = train(model, training_loader_2, optimizer, loss_func, scheduler, epoch+1)\n",
        "    train_end_time = time.monotonic()\n",
        "\n",
        "\n",
        "    print(f\"train loss={train_loss}, epoch={epoch}\")\n",
        "\n",
        "    # val_start_time = time.monotonic()\n",
        "    # val_loss, val_acc = evaluate(model, val_loader, loss_func)\n",
        "    # val_end_time = time.monotonic()\n",
        "    #\n",
        "    # print(f\"train loss={train_loss}, epoch={epoch}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 252, 252]           4,800\n",
            "       BatchNorm2d-2         [-1, 64, 252, 252]             128\n",
            "              Tanh-3         [-1, 64, 252, 252]               0\n",
            "            Conv2d-4         [-1, 64, 248, 248]         102,400\n",
            "       BatchNorm2d-5         [-1, 64, 248, 248]             128\n",
            "              Tanh-6         [-1, 64, 248, 248]               0\n",
            "         MaxPool2d-7         [-1, 64, 124, 124]               0\n",
            "       Interpolate-8          [-1, 3, 128, 128]               0\n",
            "            Conv2d-9         [-1, 64, 126, 126]           1,728\n",
            "      BatchNorm2d-10         [-1, 64, 126, 126]             128\n",
            "             Tanh-11         [-1, 64, 126, 126]               0\n",
            "           Conv2d-12         [-1, 64, 124, 124]          36,864\n",
            "             Tanh-13         [-1, 64, 124, 124]               0\n",
            "        Group1_B1-14        [-1, 128, 124, 124]               0\n",
            "           Conv2d-15        [-1, 128, 122, 122]         147,584\n",
            "             Tanh-16        [-1, 128, 122, 122]               0\n",
            "           Conv2d-17        [-1, 128, 120, 120]         147,584\n",
            "             Tanh-18        [-1, 128, 120, 120]               0\n",
            "        MaxPool2d-19          [-1, 128, 60, 60]               0\n",
            "      Interpolate-20            [-1, 3, 64, 64]               0\n",
            "           Conv2d-21          [-1, 128, 62, 62]           3,456\n",
            "      BatchNorm2d-22          [-1, 128, 62, 62]             256\n",
            "             Tanh-23          [-1, 128, 62, 62]               0\n",
            "           Conv2d-24          [-1, 128, 60, 60]         147,456\n",
            "             Tanh-25          [-1, 128, 60, 60]               0\n",
            "        Group1_B2-26          [-1, 256, 60, 60]               0\n",
            "           Conv2d-27          [-1, 256, 58, 58]         590,080\n",
            "             Tanh-28          [-1, 256, 58, 58]               0\n",
            "           Conv2d-29          [-1, 256, 56, 56]         590,080\n",
            "             Tanh-30          [-1, 256, 56, 56]               0\n",
            "        MaxPool2d-31          [-1, 256, 28, 28]               0\n",
            "      Interpolate-32            [-1, 3, 32, 32]               0\n",
            "           Conv2d-33          [-1, 256, 30, 30]           6,912\n",
            "      BatchNorm2d-34          [-1, 256, 30, 30]             512\n",
            "             Tanh-35          [-1, 256, 30, 30]               0\n",
            "           Conv2d-36          [-1, 256, 28, 28]         589,824\n",
            "             Tanh-37          [-1, 256, 28, 28]               0\n",
            "        Group1_B3-38          [-1, 512, 28, 28]               0\n",
            "           Conv2d-39          [-1, 512, 26, 26]       2,359,808\n",
            "             Tanh-40          [-1, 512, 26, 26]               0\n",
            "           Conv2d-41          [-1, 512, 24, 24]       2,359,808\n",
            "             Tanh-42          [-1, 512, 24, 24]               0\n",
            "        MaxPool2d-43          [-1, 512, 12, 12]               0\n",
            "      Interpolate-44            [-1, 3, 16, 16]               0\n",
            "           Conv2d-45          [-1, 512, 14, 14]          13,824\n",
            "      BatchNorm2d-46          [-1, 512, 14, 14]           1,024\n",
            "             Tanh-47          [-1, 512, 14, 14]               0\n",
            "           Conv2d-48          [-1, 512, 12, 12]       2,359,296\n",
            "             Tanh-49          [-1, 512, 12, 12]               0\n",
            "        Group1_B4-50         [-1, 1024, 12, 12]               0\n",
            "           Conv2d-51         [-1, 2048, 10, 10]      18,876,416\n",
            "             Tanh-52         [-1, 2048, 10, 10]               0\n",
            "           Conv2d-53           [-1, 2048, 8, 8]      37,750,784\n",
            "             Tanh-54           [-1, 2048, 8, 8]               0\n",
            "        Group2_B5-55           [-1, 2048, 8, 8]               0\n",
            "  ConvTranspose2d-56         [-1, 1024, 16, 16]       8,389,632\n",
            "           Conv2d-57         [-1, 1024, 14, 14]       9,438,208\n",
            "             Tanh-58         [-1, 1024, 14, 14]               0\n",
            "           Conv2d-59         [-1, 1024, 12, 12]       9,438,208\n",
            "             Tanh-60         [-1, 1024, 12, 12]               0\n",
            "  ConvTranspose2d-61         [-1, 1024, 16, 16]      26,215,424\n",
            "  ConvTranspose2d-62         [-1, 1024, 16, 16]      26,215,424\n",
            "           Conv2d-63         [-1, 1024, 16, 16]      18,875,392\n",
            "             Tanh-64         [-1, 1024, 16, 16]               0\n",
            "        Group3_Bi-65         [-1, 1024, 16, 16]               0\n",
            "  ConvTranspose2d-66          [-1, 512, 32, 32]       2,097,664\n",
            "           Conv2d-67          [-1, 512, 30, 30]       2,359,808\n",
            "             Tanh-68          [-1, 512, 30, 30]               0\n",
            "           Conv2d-69          [-1, 512, 28, 28]       2,359,808\n",
            "             Tanh-70          [-1, 512, 28, 28]               0\n",
            "  ConvTranspose2d-71          [-1, 512, 32, 32]       6,554,112\n",
            "  ConvTranspose2d-72          [-1, 512, 32, 32]       6,554,112\n",
            "           Conv2d-73          [-1, 512, 32, 32]       4,719,104\n",
            "             Tanh-74          [-1, 512, 32, 32]               0\n",
            "        Group3_Bi-75          [-1, 512, 32, 32]               0\n",
            "  ConvTranspose2d-76          [-1, 256, 64, 64]         524,544\n",
            "           Conv2d-77          [-1, 256, 62, 62]         590,080\n",
            "             Tanh-78          [-1, 256, 62, 62]               0\n",
            "           Conv2d-79          [-1, 256, 60, 60]         590,080\n",
            "             Tanh-80          [-1, 256, 60, 60]               0\n",
            "  ConvTranspose2d-81          [-1, 256, 64, 64]       1,638,656\n",
            "  ConvTranspose2d-82          [-1, 256, 64, 64]       1,638,656\n",
            "           Conv2d-83          [-1, 256, 64, 64]       1,179,904\n",
            "             Tanh-84          [-1, 256, 64, 64]               0\n",
            "        Group3_Bi-85          [-1, 256, 64, 64]               0\n",
            "  ConvTranspose2d-86        [-1, 128, 128, 128]         131,200\n",
            "           Conv2d-87        [-1, 128, 126, 126]         147,584\n",
            "             Tanh-88        [-1, 128, 126, 126]               0\n",
            "           Conv2d-89        [-1, 128, 124, 124]         147,584\n",
            "             Tanh-90        [-1, 128, 124, 124]               0\n",
            "  ConvTranspose2d-91        [-1, 128, 128, 128]         409,728\n",
            "  ConvTranspose2d-92        [-1, 128, 128, 128]         409,728\n",
            "           Conv2d-93        [-1, 128, 128, 128]         295,040\n",
            "             Tanh-94        [-1, 128, 128, 128]               0\n",
            "        Group3_Bi-95        [-1, 128, 128, 128]               0\n",
            "  ConvTranspose2d-96         [-1, 64, 256, 256]          32,832\n",
            "           Conv2d-97         [-1, 64, 256, 256]          36,928\n",
            "             Tanh-98         [-1, 64, 256, 256]               0\n",
            "        Dropout2d-99         [-1, 64, 256, 256]               0\n",
            "          Conv2d-100          [-1, 6, 256, 256]           3,462\n",
            "      Group4_Pa1-101  [[-1, 6, 256, 256], [-1, 64, 256, 256]]               0\n",
            " ConvTranspose2d-102        [-1, 128, 256, 256]         524,416\n",
            "          Conv2d-103        [-1, 128, 256, 256]         147,584\n",
            "            Tanh-104        [-1, 128, 256, 256]               0\n",
            "       Dropout2d-105        [-1, 128, 256, 256]               0\n",
            "          Conv2d-106          [-1, 6, 256, 256]           6,918\n",
            "      Group4_Pa2-107  [[-1, 6, 256, 256], [-1, 128, 256, 256]]               0\n",
            " ConvTranspose2d-108        [-1, 256, 256, 256]       8,388,864\n",
            "          Conv2d-109        [-1, 256, 256, 256]         590,080\n",
            "            Tanh-110        [-1, 256, 256, 256]               0\n",
            "       Dropout2d-111        [-1, 256, 256, 256]               0\n",
            "          Conv2d-112          [-1, 6, 256, 256]          13,830\n",
            "      Group4_Pa3-113  [[-1, 6, 256, 256], [-1, 256, 256, 256]]               0\n",
            "       Dropout2d-114        [-1, 448, 256, 256]               0\n",
            "          Conv2d-115          [-1, 6, 256, 256]          24,198\n",
            "          Group5-116          [-1, 6, 256, 256]               0\n",
            "================================================================\n",
            "Total params: 206,779,672\n",
            "Trainable params: 206,779,672\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.75\n",
            "Forward/backward pass size (MB): 88078570.63\n",
            "Params size (MB): 788.80\n",
            "Estimated Total Size (MB): 88079360.18\n",
            "----------------------------------------------------------------\n",
            "Epoch-0: \n",
            "train loss=1.509:   1%|          | 14/2656 [00:44<2:19:54,  3.18s/it]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-57bb171217ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0mtrain_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_loader_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_loader_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0mtrain_end_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-57bb171217ac>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, opt, criterion, scheduler, epoch)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;31m# update weights according to gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    116\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF-G30oySA83"
      },
      "source": [
        "# **Visualizing Predictions vs Ground Truth**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "VpMogAqiSH8w",
        "outputId": "ae2e919c-5dfd-4486-8398-56af0ee30c40"
      },
      "source": [
        "#saving model\n",
        "# PATH = root_path + \"/model_UNet.pt\"\n",
        "# torch.save(model, PATH)\n",
        "\n",
        "# model = UNet().to(device)\n",
        "# PATH = root_path + \"/model_UNet.pt\"\n",
        "# model = torch.load(PATH)\n",
        "\n",
        "vis_predictions()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-01be3dc24d87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# model = torch.load(PATH)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mvis_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-2ff3c1ada469>\u001b[0m in \u001b[0;36mvis_predictions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvis_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mimage_gt_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks_gt_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_loader_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mpred_masks_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_gt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_gt_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# looping on batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-b85e9a10adb5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdblock1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 440\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking arugment for argument weight in method wrapper_thnn_conv2d_forward)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxCNIyFP1Hor"
      },
      "source": [
        "torch.cuda.empty_cache()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}