{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cancer Cell Segmentation.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOCZ0ETskAMdyhISD+s7QcH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Noy-Bo/Cancer-Cell-Segmentation/blob/noy/Cancer_Cell_Segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iadNjTwWwwTY"
      },
      "source": [
        "# Mounting Google Drive\n",
        "\n",
        "mounting drive, setting root path to PanNuke dataset, setting device to cuda, checking out GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-p9G0v8wsgvk",
        "outputId": "033a4249-8aa1-4757-aa5f-4ba1baab75c2"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68JnPIOOtM-f"
      },
      "source": [
        "import os\n",
        "root_path = '/content/gdrive/MyDrive/PanNuke' \n",
        "os.chdir(root_path)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1T8t0kxy7Z3B",
        "outputId": "15d3dd64-ca87-41fe-ff06-ca5a1b122b88"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cpu')\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "print(device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAtxgDvQ7eDJ",
        "outputId": "15c71531-16b6-437f-cfb8-cf2905e01a07"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jul 19 16:22:56 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    24W / 300W |      2MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbWI64yG2wEw"
      },
      "source": [
        "# **Dataset**\n",
        "loading files, creating dataloaders, etc..\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y25bHcYNtcmu"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "\n",
        "# VISUALIZING SAMPLES - note that this is built for input that is the model's output, for raw input cancel moveaxis\n",
        "def vis_sample(image, masks):\n",
        "    fig, axs = plt.subplots(1,7)\n",
        "    fig.tight_layout()\n",
        "    axs[0].imshow(np.moveaxis(image.numpy().astype(np.uint8),0,-1)); axs[0].set_title(\"Sample\")\n",
        "    masks = np.moveaxis(masks.numpy().astype(np.uint8),0,-1)\n",
        "    axs[1].imshow(masks[:,:,0], cmap=\"gray\"); axs[1].set_title(\"Neoplastic cells\")\n",
        "    axs[2].imshow(masks[:,:,1], cmap=\"gray\"); axs[2].set_title(\"Inflammatory\")\n",
        "    axs[3].imshow(masks[:,:,2], cmap=\"gray\"); axs[3].set_title(\"Connective/Soft tissue cells\")\n",
        "    axs[4].imshow(masks[:,:,3], cmap=\"gray\"); axs[4].set_title(\"Dead Cells\")\n",
        "    axs[5].imshow(masks[:,:,4], cmap=\"gray\"); axs[5].set_title(\"Epithelial\")\n",
        "    axs[6].imshow(masks[:,:,5], cmap=\"gray\"); axs[6].set_title(\"Background\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# LOADING DATASET\n",
        "def load_dataset(dir_root, dir_images, dir_masks, training_size=0.8):\n",
        "\n",
        "    train_set = PanNuke(dir_root, dir_images, dir_masks, train=True)\n",
        "\n",
        "    # Splitting train into train/val\n",
        "    permutations = torch.randperm(len(train_set))\n",
        "    split = int(np.floor(training_size * len(train_set)))\n",
        "    training_subset = SubsetRandomSampler(permutations[:split])\n",
        "    validation_subset = SubsetRandomSampler(permutations[split:])\n",
        "\n",
        "    # Apply DataLoader over train val and test data\n",
        "    train_loader = DataLoader(train_set, sampler=training_subset, batch_size=20, num_workers=4)\n",
        "    validation_loader = DataLoader(train_set, sampler=validation_subset, batch_size=20, num_workers=4)\n",
        "\n",
        "    return train_loader, validation_loader\n",
        "\n",
        "\n",
        "\n",
        "# DATASET CLASS\n",
        "class PanNuke(Dataset):\n",
        "    def __init__(self, dir_root, dir_images, dir_masks, val=False, train=False, test=False):\n",
        "        self.images = np.load(dir_root+dir_images, mmap_mode='r')\n",
        "        self.images = np.moveaxis(self.images, -1, 1)\n",
        "        #self.images = np.copy(self.images)\n",
        "        self.masks = np.load(dir_root+dir_masks, mmap_mode='r')\n",
        "        self.masks = np.moveaxis(self.masks, -1, 1)\n",
        "        #self.masks = np.copy(self.masks)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.images.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = np.copy(self.images[idx, ...])\n",
        "        #masks = np.ceil(self.masks[idx, ...]/1000)\n",
        "        masks = np.copy(self.masks[idx, ...])\n",
        "        masks = np.ceil(masks/1000)\n",
        "        return img, masks\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzeZnwcM3BUg"
      },
      "source": [
        "# **Model**\n",
        "basic UNet model configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cyi6s5FNtjrW"
      },
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        features = init_features\n",
        "        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(\n",
        "            features * 16, features * 8, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder4 = UNet._block((features * 8) * 2, features * 8, name=\"dec4\")\n",
        "        self.upconv3 = nn.ConvTranspose2d(\n",
        "            features * 8, features * 4, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder3 = UNet._block((features * 4) * 2, features * 4, name=\"dec3\")\n",
        "        self.upconv2 = nn.ConvTranspose2d(\n",
        "            features * 4, features * 2, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder2 = UNet._block((features * 2) * 2, features * 2, name=\"dec2\")\n",
        "        self.upconv1 = nn.ConvTranspose2d(\n",
        "            features * 2, features, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n",
        "\n",
        "        self.conv_masks = nn.Conv2d(in_channels=features, out_channels=6, kernel_size=1)\n",
        "\n",
        "        # self.conv = nn.Conv2d(\n",
        "        #     in_channels=features, out_channels=out_channels, kernel_size=1\n",
        "        # )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        enc1 = self.encoder1(x)\n",
        "        enc2 = self.encoder2(self.pool1(enc1))\n",
        "        enc3 = self.encoder3(self.pool2(enc2))\n",
        "        enc4 = self.encoder4(self.pool3(enc3))\n",
        "\n",
        "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
        "\n",
        "        dec4 = self.upconv4(bottleneck)\n",
        "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
        "        dec4 = self.decoder4(dec4)\n",
        "        dec3 = self.upconv3(dec4)\n",
        "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
        "        dec3 = self.decoder3(dec3)\n",
        "        dec2 = self.upconv2(dec3)\n",
        "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
        "        dec2 = self.decoder2(dec2)\n",
        "        dec1 = self.upconv1(dec2)\n",
        "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
        "        dec1 = self.decoder1(dec1)\n",
        "        #return torch.sigmoid(self.conv(dec1))\n",
        "        return torch.sigmoid(self.conv_masks(dec1))\n",
        "\n",
        "    @staticmethod\n",
        "    def _block(in_channels, features, name):\n",
        "        return nn.Sequential(\n",
        "            OrderedDict(\n",
        "                [\n",
        "                    (\n",
        "                        name + \"conv1\",\n",
        "                        nn.Conv2d(\n",
        "                            in_channels=in_channels,\n",
        "                            out_channels=features,\n",
        "                            kernel_size=3,\n",
        "                            padding=1,\n",
        "                            bias=False,\n",
        "                        ),\n",
        "                    ),\n",
        "                    (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n",
        "                    (name + \"relu1\", nn.ReLU(inplace=True)),\n",
        "                    (\n",
        "                        name + \"conv2\",\n",
        "                        nn.Conv2d(\n",
        "                            in_channels=features,\n",
        "                            out_channels=features,\n",
        "                            kernel_size=3,\n",
        "                            padding=1,\n",
        "                            bias=False,\n",
        "                        ),\n",
        "                    ),\n",
        "                    (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n",
        "                    (name + \"relu2\", nn.ReLU(inplace=True)),\n",
        "                ]\n",
        "            )\n",
        "        )"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB1G6fPU3N7I"
      },
      "source": [
        "# **Train Class**\n",
        "includes loss function configurations, train \\ eval function, etc..\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsxsKcFftmZu",
        "outputId": "f0caf284-8d57-4d79-d8de-8ced6125bf84"
      },
      "source": [
        "import sys\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import tqdm\n",
        "\n",
        "device = torch.device('cpu')\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "print(device)\n",
        "\n",
        "class BinaryDiceLoss(nn.Module):\n",
        "    \"\"\"Dice loss of binary class\n",
        "    Args:\n",
        "        smooth: A float number to smooth loss, and avoid NaN error, default: 1\n",
        "        p: Denominator value: \\sum{x^p} + \\sum{y^p}, default: 2\n",
        "        predict: A tensor of shape [N, *]\n",
        "        target: A tensor of shape same with predict\n",
        "        reduction: Reduction method to apply, return mean over batch if 'mean',\n",
        "            return sum if 'sum', return a tensor of shape [N,] if 'none'\n",
        "    Returns:\n",
        "        Loss tensor according to arg reduction\n",
        "    Raise:\n",
        "        Exception if unexpected reduction\n",
        "    \"\"\"\n",
        "    def __init__(self, smooth=1, p=2, reduction='sum'):\n",
        "        super(BinaryDiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "        self.p = p\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, predict, target):\n",
        "        assert predict.shape[0] == target.shape[0], \"predict & target batch size don't match\"\n",
        "        predict = predict.contiguous().view(predict.shape[0], -1)\n",
        "        target = target.contiguous().view(target.shape[0], -1)\n",
        "\n",
        "        num = torch.sum(torch.mul(predict, target), dim=1) + self.smooth\n",
        "        den = torch.sum(predict.pow(self.p) + target.pow(self.p), dim=1) + self.smooth\n",
        "\n",
        "        loss = 1 - num / den\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return loss.sum()\n",
        "        elif self.reduction == 'none':\n",
        "            return loss\n",
        "        else:\n",
        "            raise Exception('Unexpected reduction {}'.format(self.reduction))\n",
        "\n",
        "\n",
        "def train_epoch(training_loader, model, optimizer, loss_function):\n",
        "\n",
        "    losses = []\n",
        "    model.train()\n",
        "    with tqdm.tqdm(total=len(training_loader), file=sys.stdout) as pbar:\n",
        "        for idx_batch, (images, masks) in enumerate(training_loader, start=1):\n",
        "\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            # calculate output\n",
        "            y_hat = model(images)\n",
        "\n",
        "            # calculate loss now:\n",
        "            optimizer.zero_grad()\n",
        "            loss = loss_function(y_hat, masks)\n",
        "            loss.backward()\n",
        "\n",
        "            # optimizing weights\n",
        "            optimizer.step()\n",
        "\n",
        "            # update loss bar\n",
        "            losses.append(loss.detach())\n",
        "            pbar.update();\n",
        "            pbar.set_description(f'train loss={losses[-1]:.3f}')\n",
        "        mean_loss = torch.mean(torch.FloatTensor(losses))\n",
        "        pbar.set_description(f'train loss={mean_loss:.3f}')\n",
        "\n",
        "        images = None\n",
        "        masks = None\n",
        "    return [mean_loss]\n",
        "\n",
        "    \n",
        "def eval_loss_epoch(training_loader, model, loss_function):\n",
        "\n",
        "    losses = []\n",
        "    model.eval()\n",
        "    with tqdm.tqdm(total=len(training_loader), file=sys.stdout) as pbar:\n",
        "        for idx_batch, (images, masks) in enumerate(training_loader, start=1):\n",
        "\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            # calculate output\n",
        "            y_hat = model(images)\n",
        "\n",
        "            # calculate loss now:\n",
        "            loss = loss_function(y_hat, masks)\n",
        "\n",
        "            # optimizing weights\n",
        "\n",
        "            # update loss bar\n",
        "            losses.append(loss.detach())\n",
        "            pbar.update();\n",
        "            pbar.set_description(f'val loss={losses[-1]:.3f}')\n",
        "\n",
        "            images = None\n",
        "            masks = None\n",
        "        mean_loss = torch.mean(torch.FloatTensor(losses))\n",
        "        pbar.set_description(f'val loss={mean_loss:.3f}')\n",
        "\n",
        "    return [mean_loss]\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgCVfGIl3jiZ"
      },
      "source": [
        "# **Main**\n",
        "- training is done here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-t83yC39t7sH",
        "outputId": "6bbc57c9-bf1a-4190-d78e-3e0aa6611bd3"
      },
      "source": [
        "from torchsummary import summary\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#pannuke = PanNuke(dir_root=root, dir_masks=masks_dir, dir_images=images_dir)\n",
        "\n",
        "# creating data loaders\n",
        "images_dir = 'Images 1/images.npy'\n",
        "masks_dir = 'Masks 1/masks.npy'\n",
        "training_loader_1, _ = load_dataset(dir_root='', dir_masks=masks_dir, dir_images=images_dir, training_size=1)\n",
        "images_dir = 'Images 3/images.npy'\n",
        "masks_dir = 'Masks 3/masks.npy'\n",
        "training_loader_2, _ = load_dataset(dir_root='', dir_masks=masks_dir, dir_images=images_dir, training_size=1)\n",
        "images_dir = 'Images 2/images.npy'\n",
        "masks_dir = 'Masks 2/masks.npy'\n",
        "validation_loader, test_loader = load_dataset(dir_root='', dir_masks=masks_dir, dir_images=images_dir, training_size=0.5)\n",
        "\n",
        "# creating model\n",
        "model = UNet().to(device)\n",
        "summary(model, (3,256,256))\n",
        "model.double()\n",
        "\n",
        "# visualization sample \n",
        "image, masks = iter(training_loader_1).next()\n",
        "vis_sample(image[0,...], masks[0,...])\n",
        "\n",
        "# config training\n",
        "epochs = 30\n",
        "loss_function = BinaryDiceLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9,0.999), eps =1e-08)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 5, gamma=0.8)\n",
        "\n",
        "# training\n",
        "for i in range (1,epochs):\n",
        " print(\"=========================== EPOCH: {} ===========================\".format(str(i)))\n",
        " print(\"TRAIN LOSS:\")\n",
        " train_epoch(training_loader=training_loader_1, model=model, optimizer=optimizer, loss_function=loss_function)\n",
        " train_epoch(training_loader=training_loader_2, model=model, optimizer=optimizer, loss_function=loss_function)\n",
        " print(\"VAL LOSS:\")\n",
        " eval_loss_epoch(training_loader=validation_loader, model=model, loss_function=loss_function)\n",
        " scheduler.step()\n",
        "\n",
        "# saving model\n",
        "PATH = root_path + \"/entire_model.pt\"\n",
        "torch.save(model, PATH)\n",
        "\n",
        "print(\"END\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 32, 256, 256]             864\n",
            "       BatchNorm2d-2         [-1, 32, 256, 256]              64\n",
            "              ReLU-3         [-1, 32, 256, 256]               0\n",
            "            Conv2d-4         [-1, 32, 256, 256]           9,216\n",
            "       BatchNorm2d-5         [-1, 32, 256, 256]              64\n",
            "              ReLU-6         [-1, 32, 256, 256]               0\n",
            "         MaxPool2d-7         [-1, 32, 128, 128]               0\n",
            "            Conv2d-8         [-1, 64, 128, 128]          18,432\n",
            "       BatchNorm2d-9         [-1, 64, 128, 128]             128\n",
            "             ReLU-10         [-1, 64, 128, 128]               0\n",
            "           Conv2d-11         [-1, 64, 128, 128]          36,864\n",
            "      BatchNorm2d-12         [-1, 64, 128, 128]             128\n",
            "             ReLU-13         [-1, 64, 128, 128]               0\n",
            "        MaxPool2d-14           [-1, 64, 64, 64]               0\n",
            "           Conv2d-15          [-1, 128, 64, 64]          73,728\n",
            "      BatchNorm2d-16          [-1, 128, 64, 64]             256\n",
            "             ReLU-17          [-1, 128, 64, 64]               0\n",
            "           Conv2d-18          [-1, 128, 64, 64]         147,456\n",
            "      BatchNorm2d-19          [-1, 128, 64, 64]             256\n",
            "             ReLU-20          [-1, 128, 64, 64]               0\n",
            "        MaxPool2d-21          [-1, 128, 32, 32]               0\n",
            "           Conv2d-22          [-1, 256, 32, 32]         294,912\n",
            "      BatchNorm2d-23          [-1, 256, 32, 32]             512\n",
            "             ReLU-24          [-1, 256, 32, 32]               0\n",
            "           Conv2d-25          [-1, 256, 32, 32]         589,824\n",
            "      BatchNorm2d-26          [-1, 256, 32, 32]             512\n",
            "             ReLU-27          [-1, 256, 32, 32]               0\n",
            "        MaxPool2d-28          [-1, 256, 16, 16]               0\n",
            "           Conv2d-29          [-1, 512, 16, 16]       1,179,648\n",
            "      BatchNorm2d-30          [-1, 512, 16, 16]           1,024\n",
            "             ReLU-31          [-1, 512, 16, 16]               0\n",
            "           Conv2d-32          [-1, 512, 16, 16]       2,359,296\n",
            "      BatchNorm2d-33          [-1, 512, 16, 16]           1,024\n",
            "             ReLU-34          [-1, 512, 16, 16]               0\n",
            "  ConvTranspose2d-35          [-1, 256, 32, 32]         524,544\n",
            "           Conv2d-36          [-1, 256, 32, 32]       1,179,648\n",
            "      BatchNorm2d-37          [-1, 256, 32, 32]             512\n",
            "             ReLU-38          [-1, 256, 32, 32]               0\n",
            "           Conv2d-39          [-1, 256, 32, 32]         589,824\n",
            "      BatchNorm2d-40          [-1, 256, 32, 32]             512\n",
            "             ReLU-41          [-1, 256, 32, 32]               0\n",
            "  ConvTranspose2d-42          [-1, 128, 64, 64]         131,200\n",
            "           Conv2d-43          [-1, 128, 64, 64]         294,912\n",
            "      BatchNorm2d-44          [-1, 128, 64, 64]             256\n",
            "             ReLU-45          [-1, 128, 64, 64]               0\n",
            "           Conv2d-46          [-1, 128, 64, 64]         147,456\n",
            "      BatchNorm2d-47          [-1, 128, 64, 64]             256\n",
            "             ReLU-48          [-1, 128, 64, 64]               0\n",
            "  ConvTranspose2d-49         [-1, 64, 128, 128]          32,832\n",
            "           Conv2d-50         [-1, 64, 128, 128]          73,728\n",
            "      BatchNorm2d-51         [-1, 64, 128, 128]             128\n",
            "             ReLU-52         [-1, 64, 128, 128]               0\n",
            "           Conv2d-53         [-1, 64, 128, 128]          36,864\n",
            "      BatchNorm2d-54         [-1, 64, 128, 128]             128\n",
            "             ReLU-55         [-1, 64, 128, 128]               0\n",
            "  ConvTranspose2d-56         [-1, 32, 256, 256]           8,224\n",
            "           Conv2d-57         [-1, 32, 256, 256]          18,432\n",
            "      BatchNorm2d-58         [-1, 32, 256, 256]              64\n",
            "             ReLU-59         [-1, 32, 256, 256]               0\n",
            "           Conv2d-60         [-1, 32, 256, 256]           9,216\n",
            "      BatchNorm2d-61         [-1, 32, 256, 256]              64\n",
            "             ReLU-62         [-1, 32, 256, 256]               0\n",
            "           Conv2d-63          [-1, 6, 256, 256]             198\n",
            "================================================================\n",
            "Total params: 7,763,206\n",
            "Trainable params: 7,763,206\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.75\n",
            "Forward/backward pass size (MB): 406.50\n",
            "Params size (MB): 29.61\n",
            "Estimated Total Size (MB): 436.86\n",
            "----------------------------------------------------------------\n",
            "=========================== EPOCH: 1 ===========================\n",
            "TRAIN LOSS:\n",
            "\r  0%|          | 0/133 [00:00<?, ?it/s]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss=14.964: 100%|██████████| 133/133 [06:55<00:00,  3.12s/it]\n",
            "train loss=14.098: 100%|██████████| 137/137 [06:54<00:00,  3.03s/it]\n",
            "VAL LOSS:\n",
            "val loss=13.635: 100%|██████████| 64/64 [04:07<00:00,  3.86s/it]\n",
            "=========================== EPOCH: 2 ===========================\n",
            "TRAIN LOSS:\n",
            "train loss=13.479: 100%|██████████| 133/133 [03:46<00:00,  1.71s/it]\n",
            "train loss=12.955: 100%|██████████| 137/137 [03:51<00:00,  1.69s/it]\n",
            "VAL LOSS:\n",
            "val loss=12.580: 100%|██████████| 64/64 [01:31<00:00,  1.43s/it]\n",
            "=========================== EPOCH: 3 ===========================\n",
            "TRAIN LOSS:\n",
            "train loss=12.494: 100%|██████████| 133/133 [03:46<00:00,  1.70s/it]\n",
            "train loss=12.172:  23%|██▎       | 32/137 [00:57<02:54,  1.66s/it]"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}